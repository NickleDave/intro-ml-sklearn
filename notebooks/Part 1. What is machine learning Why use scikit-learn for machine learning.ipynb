{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1. What is machine learning? Why use scikit-learn for machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is machine learning?\n",
    "* everday examples:\n",
    "  - email spam filter\n",
    "  - Netflix or Spotify recommendations\n",
    "* \"gives computers ability to learn without being programmed\"\n",
    "  --Arthur Samuel, inventor of the term \"machine learning\" and pioneering researcher\n",
    "* automates tasks for which it would be tedious or just imposible to write imperative code\n",
    "  - examples above ~ tedious\n",
    "  - games, such as chess ~ mostly impossible\n",
    "* So if we program a computer with machine learning, it should do two things:\n",
    "  1. let the computer make decisions **without explicit rules**\n",
    "  2. **generalize** so that it still makes correct decisions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Broad categories of machine learning\n",
    "    \n",
    "1. Supervised Learning\n",
    "   - Classification\n",
    "       common algorithms: k-nearest neighbors, support vector machine, random forests\n",
    "   - Regression\n",
    "2. Unsupervised Learning\n",
    "   - Clustering\n",
    "   - Dimensionality Reduction\n",
    "3. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Machine learning is just modeling\n",
    "\n",
    "* ML algorithms generate a **model** $y = f(x)$\n",
    "    - you **\"train\"** the model with data you’ve seen and an **algorithm** that generates the model\n",
    "    - then you **predict** based on new data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Types of supervised learning: regression\n",
    "\n",
    "In this tutorial we will focus on supervised learning. These algorithms are the best understood in the field of machine learning and are the most commonly used in practical applications.\n",
    "\n",
    "Scientists tend to think of linear regression as a way to show a relationship between two variables.\n",
    "\n",
    "But a linear regression also functions as a model that makes predictions.\n",
    "\n",
    "To emphasize that machine learning is just modeling, I will walk through the process of fitting a linear regression model using scikit-learn. This allows me to introduce the scikit-learn API as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generate some fake data\n",
    "from sklearn.datasets.samples_generator import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(random_state=42)\n",
    "# split fake data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[1,:].reshape(-1,1), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import model class\n",
    "from sklearn import linear_model\n",
    "\n",
    "# instantiate model, i.e., create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model, i.e., train using the training sets\n",
    "regr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tell notebook to show plots inline instead of opening in a separate window\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "seaborn.set_style(\"darkgrid\", {\"legend.frameon\": \"True\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we pick a point on the x axis for which we do not have data, our model gives us a predicted value $\\hat{y}$ for that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.5,7.5))\n",
    "p1 = plt.scatter(X_train, y_train, color='blue',label='data')\n",
    "p2, = plt.plot(X_train, regr.predict(X_train), color='black',\n",
    "         linewidth=3,label='model')\n",
    "y_hat = regr.predict(X_test[0,0])\n",
    "p3, = plt.plot((X_test[0,0],X_test[0,0]),(plt.ylim()[0],y_hat),'r--',linewidth=3,label='new data point')\n",
    "p4, = plt.plot(X_test[0,0],y_hat,'ro',markersize=10,label='predicted value')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend([p1,p2,p3,p4],('data','model','new data point','predicted value'),fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**So what?**\n",
    "\n",
    "Bear in mind that machine learning algorithms let you can do this with 100s of observations per label instead of just one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Types of supervised learning: classification\n",
    "\n",
    "In classification, the $y$ values in our training data set are not continuous values.\n",
    "\n",
    "Instead, each training sample $x$ will have a label $y$ that belongs to a set of **classes**.\n",
    "\n",
    "Below is another fake data example.\n",
    "\n",
    "This data has two classes: pink dots and brown dots.\n",
    "\n",
    "The support vector machine (who we will meet again later) is an algorithm that finds a dividing line (solid black line) that maximizes the margin (dashed black line) between two groups.\n",
    "\n",
    "It represents all those lines with just the data points that fall on them. Those data points are the support vectors.\n",
    "\n",
    "Anything that falls in the area above the top dashed line will be classified in the 'pink dot' class, while anything below the bottom dashed line will be classified as belonging to the 'brown dot' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from fig_code import plot_svm_separator\n",
    "plot_svm_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the example above looks like something we could do by eye.\n",
    "\n",
    "But when each data point is a vector with thousands of features, we can't easily draw a line between groups.\n",
    "\n",
    "But a machine learning algorithm often can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is scikit-learn and why should I use it?\n",
    "\n",
    "[Scikit-Learn](http://scikit-learn.org/stable/index.html):\n",
    " * Python package\n",
    " * provides access to well-known machine learning algorithms via Python code\n",
    " * through a clean, well-thought-out API\n",
    " * as opposed to you trying to install research code written in different low-level languages and then figuring out how to use it\n",
    " * built upon Python's NumPy (Numerical Python) and SciPy (Scientific Python) libraries\n",
    " * not specifically designed for extremely large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Representation of data in machine learning and scikit-learn\n",
    "\n",
    "Recall that we are creating a model, $y = f(x)$.\n",
    "\n",
    "Specifically, when we train a supervised learning algorithm for classification:\n",
    " - we train that model with **data** $x$ and **labels** $y$.\n",
    " - we want to be able to enter some new data $x$ in our model $f()$ and predict the correct label $y$.\n",
    "\n",
    "**Data** $x$ will take the form of 2-d matrix.\n",
    "\n",
    "Since scikit-learn is built on top of the numpy package, it uses numpy $n$-dimensional arrays to represent data.\n",
    "\n",
    "In numpy, we can get the number of rows and columns from the `shape` property of an array.\n",
    "\n",
    "Like so:\n",
    "\n",
    "`>>>number_of_rows, number_of_columns = X.shape`\n",
    "\n",
    "where:\n",
    " - `number_of_rows` = number of **samples**, i.e., discrete items you’re interested in looking at\n",
    "   * e.g. if you were classifying images, each image would be a sample\n",
    "\n",
    " - `number_of_columns` = number of **features**\n",
    "   * where each features is an observation/measurement you have for every sample\n",
    "\n",
    "The scikit-learn API expects your **labels** $y$ to be in the form of a 1-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example: the iris dataset\n",
    "\n",
    "\"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis*. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\"  -- [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "\n",
    "<img src=\"./images/iris_setosa.jpg\",style=\"height: 250px; display: inline;\"/>\n",
    "<img src=\"./images/iris_versicolor.jpg\",style=\"height: 250px; display: inline;\"/>\n",
    "<img src=\"./images/iris_virginica.jpg\",style=\"height: 250px; display: inline;\"/>\n",
    "\n",
    "(left to right: Iris Setosa, Versicolor, and Virginica)\n",
    "\n",
    "The Iris dataset comes with scikit-learn. We can load it with the `load_iris` function in the `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.keys()) # iris is basically a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, again, the iris data is just a matrix, like so:\n",
    "    \n",
    "$$\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{1}^{(1)} & x_{2}^{(1)} & x_{3}^{(1)} & \\dots  & x_{4}^{(1)} \\\\\n",
    "    x_{1}^{(2)} & x_{2}^{(2)} & x_{3}^{(2)} & \\dots  & x_{4}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{1}^{(150)} & x_{2}^{(150)} & x_{3}^{(150)} & \\dots  & x_{4}^{(150)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "(The superscript denotes the *i*th row/sample, and the subscript denotes the *j*th feature/column, respectively.)\n",
    "\n",
    "** what are the features? **\n",
    "\n",
    "<img src=\"./images/iris_petal_sepal.png\" ,style=\"height: 300px; display: inline;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "# the sepal length, sepal width, petal length and petal width of the first sample (first flower)\n",
    "print('Feature values for the first sample in the data set:')\n",
    "for name, value in zip(iris['feature_names'],iris.data[0]):\n",
    "    print('{}: {}'.format(name,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Scale your data!\n",
    "\n",
    "Notice that all these features have the same units and are the same order of magnitude. That's not typically the case!\n",
    "\n",
    "Different scales for different features can cause problems. For example, if one of the features you're measuring has values that are all orders of magnitude larger than the others, it may appear to do a better job of separating the classes. But if you transform the data in some way so that all the features are on the same order of magnitude, it might turn out that other features or combinations of features do a better job of separating classes.\n",
    "\n",
    "This is why it's almost always a good idea to **scale** your data.\n",
    "\n",
    "A typical approach to scaling is to subtract the mean of every feature so that the values are centered around 0, and then divide by the standard deviation. Basically you're converting values to z-scores.\n",
    "\n",
    "scikit-learn has a convenience function that does this for you:\n",
    "\n",
    "`sklearn.preprocessing.scale`\n",
    "\n",
    "It's also really important that you transform your **testing** data **with the means and standard deviations from your *training* data**.\n",
    "\n",
    "Luckily scikit-learn makes this easy as well.\n",
    "\n",
    "The `preprocessing` module includes a `StandardScaler` class that you can `fit` just like you fit estimators in scikit-learn.\n",
    "After you `fit`, you then `transform`.\n",
    "\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train)\n",
    "X_train_scaled = sclr.transform(X_train)\n",
    "# you can do the above two lines in one step with `fit_transform()`\n",
    "X_test_scaled = sclr.transform(X_test)\n",
    "```\n",
    "\n",
    "You can read [more about scaling](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) in the scikit-learn docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels and classes\n",
    "\n",
    "Remember that **labels** should take the form of a 1-d vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iris['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice also that scikit-learn expects your class labels to be integers.\n",
    "\n",
    "It is not set up to deal with labels as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(iris['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So you'll need some variable that maps from integers to class names.\n",
    "\n",
    "For example, a list where the indices are the integers you use to represent each class.\n",
    "\n",
    "I.e., if I represent the class 'setosa' with integer `0` then I should have `target_names[0] = 'setosa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** It is always a good idea to plot your raw data. **\n",
    "\n",
    "** *Especially before you spend months trying to apply a machine learning algorithm to it!!!* **\n",
    "\n",
    "Let's plot the iris dataset.\n",
    "It's four dimensional but we can visualize two dimensions at a time with a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_index = 0\n",
    "y_index = 3\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.figure(figsize=(7.5,7.5))\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('spring', 3))\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(iris.feature_names[x_index],fontsize=16)\n",
    "plt.ylabel(iris.feature_names[y_index],fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## the scikit-learn API\n",
    "\n",
    "Python is an **object-oriented** language.\n",
    "\n",
    "Remember / know that in object-oriented languages, we create **\"classes\"** of objects that have certain **methods** and **properties**, and we can then make **instances** of those class, usually by assigning the class to a variable.\n",
    "\n",
    "In scikit-learn, each type of model/algorithm is a class.\n",
    "\n",
    "Let me generate some more fake data so I can demonstrate the basic steps you'll follow every time you use scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic outline for using a scikit-learn estimator:\n",
    "\n",
    "1.Import the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Instantiate the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(By `print`ing the model, we can inspect its properties. Those properties include the **hyperparameters** that dictate how the algorithm uses the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.You fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you fit a model object with data, it will have new properties.\n",
    "\n",
    "Those properties are the parameters of the model.\n",
    "\n",
    "(By convention, the names of the properties added after a fit all end with an underscore.)\n",
    "\n",
    "For example, after fitting a support vector machine, it will have the property `support_vectors_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.You predict new y values, given new x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: the scikit-learn estimator interface\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods.\n",
    "\n",
    "Given a scikit-learn *estimator* object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
